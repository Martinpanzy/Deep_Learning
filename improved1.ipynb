{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "improved.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Martinpanzy/Deep_Learning/blob/master/improved1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "scylC1qNuBY0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Baseline Code\n",
        "\n",
        "This code introduces a two-step training for the N-HPatches problem. In N-HPatches problem, we aim to generate a patch descriptor that is able to perform successfully tasks such as matching, retrieval or verification. \n",
        "\n",
        "Contrary to classical HPatches dataset, in N-HPatches, images contain random non-smooth perturbations produced by a synthetic noise. This noise could be critical when training the descriptor, therefore, we introduce a denoising model that could help us to deal with those perturbations. Denoising models have been already introduced in the course [tutorials](https://github.com/MatchLab-Imperial/deep-learning-course) and lectures, their objective is to generate a clean/denoised version of the input image.  We will refer in this code to the images with noise as `noisy`, to the images after applying the denoise model as `denoised` and the original patches from HPatches (so no extra noise added) which are used as ground-truth for the denoising step as `clean`. \n",
        "\n",
        "\n",
        "Thus, we aim to minimize the noise in images before the second step, which is computing a feature vector, also called descriptor. Those descriptions must be a powerful representation of the input patches. The idea behind is that if two descriptors belong two similar patches, they should be close to each other, i.e. have a low Euclidean distance. See figure below:\n",
        "\n",
        "![](https://i.ibb.co/4tvm3Vh/descriptorspace.png)\n",
        "\n",
        "This baseline code gives a method you can use to compare to whatever another approach you develop.  There are several other approaches you can test to see if there is any improvement, e.g. train the descriptor directly with noisy patches, without the denoising model. However, this code provides some guidance about how to implement the different blocks, how to stack them if desired, how to read the data and how to evaluate the method.\n",
        "\n",
        "The values given can be improved without changing the core method, only by tuning correctly the hyperparameters or giving it more training time, among others.\n",
        "\n",
        "As a first step of the project, you should get familiar with the problem and the provided code, so you can develop more complex and robust algorithms afterward. "
      ]
    },
    {
      "metadata": {
        "id": "iamuRgeiNLjW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Safety Check\n",
        "\n",
        "As Google Colab is an external platform, we cannot guarantee that everytime you connect to a remote server, you will have the same amount of RAM or video RAM. For that reason, we will first check the amount of memory we have in the notebook. RAM should be around 12.9 GB, which is enough to load the datasets in memory. Also, usually, we have available 11.4 GB of GPU memory, which is more than enough to run this code. However, some users reported having only 500 MB of GPU memory. If you have that amount, restart the environment to see if you get the corresponding 11.4 GB."
      ]
    },
    {
      "metadata": {
        "id": "ZZG4BqkENEyd",
        "colab_type": "code",
        "outputId": "787c4ae3-755c-4599-9e10-77976b85e10c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "cell_type": "code",
      "source": [
        "# Taken from\n",
        "# https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# Colab only provides one GPU and it is not always guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python2.7/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BBvIvBoyg68g",
        "colab_type": "code",
        "outputId": "ebb884de-9ef7-446d-f2b6-05d8d7216bfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "printm()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('RAM Free: 12.9 GB', ' | Proc size: 152.1 MB')\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OMiynJ7p-zI8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Downloading Functions and Data\n",
        "\n",
        "The first step is to clone the GitHub repository of the course, which contains already implemented functions. You can use your own function and import them here doing the same. In addition, we are going to download and extract the N-HPatches data. \n",
        "\n",
        "As a note, in colab, we can run terminal commands by using ```!```. Also, by using ```%``` we have access to the [built-in IPython magic commands](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-cd), which we will use to move through directories (`cd` command). It takes around 5 minutes to download and unzip the dataset. \n"
      ]
    },
    {
      "metadata": {
        "id": "yV1m-9ZGuKGj",
        "colab_type": "code",
        "outputId": "777e8365-cdbe-472d-c1a9-4506992feb3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "cell_type": "code",
      "source": [
        "# Clone repo\n",
        "!git clone https://github.com/MatchLab-Imperial/keras_triplet_descriptor"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras_triplet_descriptor'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 181 (delta 0), reused 1 (delta 0), pack-reused 178\u001b[K\n",
            "Receiving objects: 100% (181/181), 149.87 MiB | 18.74 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (69/69), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pyZSqhZ5LACT",
        "colab_type": "code",
        "outputId": "1def661a-bf47-4cb3-f98b-75b7c4efd1d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Change directory\n",
        "%cd /content/keras_triplet_descriptor    \n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/keras_triplet_descriptor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "307CBCL-FjX4",
        "colab_type": "code",
        "outputId": "5389ab96-99c3-4506-8bb9-d263aa4a7afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "cell_type": "code",
      "source": [
        "# Download data\n",
        "!wget -O hpatches_data.zip https://imperialcollegelondon.box.com/shared/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-22 15:16:55--  https://imperialcollegelondon.box.com/shared/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Resolving imperialcollegelondon.box.com (imperialcollegelondon.box.com)... 185.235.236.197\n",
            "Connecting to imperialcollegelondon.box.com (imperialcollegelondon.box.com)|185.235.236.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip [following]\n",
            "--2019-03-22 15:16:55--  https://imperialcollegelondon.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Reusing existing connection to imperialcollegelondon.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://imperialcollegelondon.app.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip [following]\n",
            "--2019-03-22 15:16:55--  https://imperialcollegelondon.app.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Resolving imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)... 185.235.236.199\n",
            "Connecting to imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)|185.235.236.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!8il3Hpz2t45FB3V1lDImJt0hQU8pnXj_Yj4dQf2bDZb4xQjz8C0LHjZRveuAhE3gUMSNnJ7JEJnS5kgZVZPCM4c_xLj3CMvNqhq-hffnH95eAgncLSexEWMyBpZeI_1l-Gc99pxHYyRG2oth9zYlCv7xzX7EHImfOrOaot0jFlDHtx-HEfY_0eevtEEsG6zzy0PfyJryc11gMy96qv5-k2zh7uuctAgvoB-o8GsLizkAtTaH8FAeZAOAB7wJmcHEsd7roA-syKrJxLRm-hgTW9O-Hd3Tlk7jeCwbtN-d_2k4VQhCrhAl2vJfRfGcbZfz2cuTfw6gmiyz1OZtWl_UbuUrLJQtnkL6zga0Q9gOYc-Z_AePcwkyRUyMNnzfj9Zggci1IdKqzTr2tz2rxDCoYT18CEznN_px2AvSRyFJk_zq441FdtDAmg0w4Qb5cg1l8lgI6pV1Bk4qPk8NDoX3j1-hgpDp6WQbfwMV1sQuv9BBHZbhfI3Gb4MT_6hbZtdrzjMNubyI2Y0wmE1sbkZbcY9I75RDSYrGlv43DZjkMXpLcCqRfA13enc8XNuPQsaGvyYtigz4awVxcdnKLtR2-X5fb23iUbYXo1YCXWSZGvCMTsjqQlqF5XraBoIKrI7NHT0UIGLWM_UnVfnMGuvvrs2kdrBkx-nr6IUUunJBFLfun3Txb16zWFT18vIprVjWfI5v8inZO4JGflp_vNTeUfuj_HmKHtfCsPnaXtqttEFABgNYOGsaSsi6kK7Hx4gv9LSY8p8wBiAzW3YbiZn9Vz1zvaq6tTryt5lZxsN2bzM_Lr5p_0YcdIn-5Wjs0Bz0-7nz79ecERmQbFmruGYZ9De95mSvOBOTZMF2TyCZR_kqEcmz4ww63wKDiICrs3tZJSeN7BwhHmD001cMgww0KHNQuhqpBFlayn3L2v0h5awy3eYkcX-KzMqGx60fD69SNvFEL8Eb-NuvfcG_IlPkb4OW-nyd8kUfAI8tmoIQoUKBJlwy0zFQVKb6FAKfVdw5xCNS5nzo3trPfbBheugZY9JH1EhM8GnOBAC3xxk7u9j6v20KExj_P73Hz70LMSMlRhHEvvuKowFNaxzhHMPHM8e0ajIMND7Nmi3k-RjoNBWy2hblPN0dkhnLPkLO6ql7O9NlAwytOoXXuoCjmtTB1zjTh2hKBuKrdU_l_DSJmntPKutY9KEXML9M2JBZpZFDM7qvV4XloaJgyEupONp60gvzX3LCq2B-ZzuZAEIibnEB76XzWSDewuUvSU1GT9nryJmRrXgYlylt4vaQI1amaAv96Qbr-eQcs2g0ypLKaoHQI2fCqZZwBGLiC8X7MvKYUgjQ93W9qA0maB05ERd7NzaOqKs6OI_o7UPvtm9t1dxt2bee52zagXPQJYDafeJD3quxlKUTwYt_3yjwtRIiPO1lDxsljrMtxnqwIIZLu3JJnntwlkI2QzqNtloW_WsgmuO0RLvtb_Ml6M1JEoD2Gj7A89l0M-2Xu14./download [following]\n",
            "--2019-03-22 15:16:56--  https://public.boxcloud.com/d/1/b1!8il3Hpz2t45FB3V1lDImJt0hQU8pnXj_Yj4dQf2bDZb4xQjz8C0LHjZRveuAhE3gUMSNnJ7JEJnS5kgZVZPCM4c_xLj3CMvNqhq-hffnH95eAgncLSexEWMyBpZeI_1l-Gc99pxHYyRG2oth9zYlCv7xzX7EHImfOrOaot0jFlDHtx-HEfY_0eevtEEsG6zzy0PfyJryc11gMy96qv5-k2zh7uuctAgvoB-o8GsLizkAtTaH8FAeZAOAB7wJmcHEsd7roA-syKrJxLRm-hgTW9O-Hd3Tlk7jeCwbtN-d_2k4VQhCrhAl2vJfRfGcbZfz2cuTfw6gmiyz1OZtWl_UbuUrLJQtnkL6zga0Q9gOYc-Z_AePcwkyRUyMNnzfj9Zggci1IdKqzTr2tz2rxDCoYT18CEznN_px2AvSRyFJk_zq441FdtDAmg0w4Qb5cg1l8lgI6pV1Bk4qPk8NDoX3j1-hgpDp6WQbfwMV1sQuv9BBHZbhfI3Gb4MT_6hbZtdrzjMNubyI2Y0wmE1sbkZbcY9I75RDSYrGlv43DZjkMXpLcCqRfA13enc8XNuPQsaGvyYtigz4awVxcdnKLtR2-X5fb23iUbYXo1YCXWSZGvCMTsjqQlqF5XraBoIKrI7NHT0UIGLWM_UnVfnMGuvvrs2kdrBkx-nr6IUUunJBFLfun3Txb16zWFT18vIprVjWfI5v8inZO4JGflp_vNTeUfuj_HmKHtfCsPnaXtqttEFABgNYOGsaSsi6kK7Hx4gv9LSY8p8wBiAzW3YbiZn9Vz1zvaq6tTryt5lZxsN2bzM_Lr5p_0YcdIn-5Wjs0Bz0-7nz79ecERmQbFmruGYZ9De95mSvOBOTZMF2TyCZR_kqEcmz4ww63wKDiICrs3tZJSeN7BwhHmD001cMgww0KHNQuhqpBFlayn3L2v0h5awy3eYkcX-KzMqGx60fD69SNvFEL8Eb-NuvfcG_IlPkb4OW-nyd8kUfAI8tmoIQoUKBJlwy0zFQVKb6FAKfVdw5xCNS5nzo3trPfbBheugZY9JH1EhM8GnOBAC3xxk7u9j6v20KExj_P73Hz70LMSMlRhHEvvuKowFNaxzhHMPHM8e0ajIMND7Nmi3k-RjoNBWy2hblPN0dkhnLPkLO6ql7O9NlAwytOoXXuoCjmtTB1zjTh2hKBuKrdU_l_DSJmntPKutY9KEXML9M2JBZpZFDM7qvV4XloaJgyEupONp60gvzX3LCq2B-ZzuZAEIibnEB76XzWSDewuUvSU1GT9nryJmRrXgYlylt4vaQI1amaAv96Qbr-eQcs2g0ypLKaoHQI2fCqZZwBGLiC8X7MvKYUgjQ93W9qA0maB05ERd7NzaOqKs6OI_o7UPvtm9t1dxt2bee52zagXPQJYDafeJD3quxlKUTwYt_3yjwtRIiPO1lDxsljrMtxnqwIIZLu3JJnntwlkI2QzqNtloW_WsgmuO0RLvtb_Ml6M1JEoD2Gj7A89l0M-2Xu14./download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 185.235.236.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|185.235.236.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4088106554 (3.8G) [application/zip]\n",
            "Saving to: ‘hpatches_data.zip’\n",
            "\n",
            "hpatches_data.zip   100%[===================>]   3.81G  18.4MB/s    in 3m 26s  \n",
            "\n",
            "2019-03-22 15:20:22 (18.9 MB/s) - ‘hpatches_data.zip’ saved [4088106554/4088106554]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "36mBTFvPCxY9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract data\n",
        "!unzip -q ./hpatches_data.zip\n",
        "!rm ./hpatches_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rjyr96hR_4wS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Importing Necessary Modules\n",
        "\n",
        "We now import the modules we will use in this baseline code. "
      ]
    },
    {
      "metadata": {
        "id": "o0KYfe-at9KN",
        "colab_type": "code",
        "outputId": "f1d158c9-1414-4308-92a4-892703d4de0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Conv2DTranspose\n",
        "from keras.layers import Input, UpSampling2D, concatenate  \n",
        "\n",
        "from read_data import HPatches, DataGeneratorDesc, hpatches_sequence_folder, DenoiseHPatches, tps\n",
        "from utils import generate_desc_csv, plot_denoise, plot_triplet\n",
        "from keras.utils import plot_model"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "AFG0LyAct_-l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The `read_data` and `utils` imports are functions provided in the repository we just cloned. You can navigate through the *File tab* and check what those functions do for a better understanding.\n",
        "\n",
        "![texto del enlace](https://i.ibb.co/HnfSvfT/filetab.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2Y61ZKWZ7o5k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We also fix the seeds of the pseudo-random number generators to have reproducible results. The idea of fixing the seed is having the same results every time the algorithm is run if there are no changes in the code."
      ]
    },
    {
      "metadata": {
        "id": "NXL31ez-AT5h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_OqFkNujBGzf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we load the data. The original HPatches dataset has several splits, which are used to separate the available sequences in train sequences and test sequences. For our experiments in N-HPatches we use the same splits as in HPatches. Specifically, we load (and report results) using the split `'a'`:\n"
      ]
    },
    {
      "metadata": {
        "id": "ABKDHB9RApZk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hpatches_dir = './hpatches'\n",
        "splits_path = './splits.json'\n",
        "\n",
        "splits_json = json.load(open(splits_path, 'rb'))\n",
        "split = splits_json['a']\n",
        "\n",
        "train_fnames = split['train']\n",
        "test_fnames = split['test']\n",
        "\n",
        "seqs = glob.glob(hpatches_dir+'/*')\n",
        "seqs = [os.path.abspath(p) for p in seqs]   \n",
        "seqs_train = list(filter(lambda x: x.split('/')[-1] in train_fnames, seqs)) \n",
        "seqs_test = list(filter(lambda x: x.split('/')[-1] in split['test'], seqs)) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qeWik0vMEtuC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Models and loss"
      ]
    },
    {
      "metadata": {
        "id": "LYJz8BDzBkIx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now define three functions that define the main modules of our baseline. \n",
        "\n",
        "*   **get_denoise_model(..)** returns the denoising model. The input for the function is the size of the patch, which will be *1x32x32*, and it outputs a keras denoising model. \n",
        "*   **get_descriptor_model(..)** builts the descriptor model. The input for the function is the size of the patch, which will be *1x32x32*, and it outputs a keras descriptor model. The model we use as baseline returns a descriptor of dimension *128x1*.\n",
        "*   **triplet_loss(..)** defines the loss function which is used to train the descriptor model. \n",
        "\n",
        "You can modify the models in these functions and run the training code again. For example, the given denoising model is quite shallow, maybe using a deeper network can improve results. Or testing new initializations for the weights. Or maybe adding dropout. Or modifying the loss function somehow..."
      ]
    },
    {
      "metadata": {
        "id": "W6QbkHnbuIUD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_denoise_model_add2(shape):\n",
        "  inputs = Input(shape)\n",
        "  ## Encoder starts\n",
        "  conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "  conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "  pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "  conv3 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)  \n",
        "  pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "  \n",
        "  conv4 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "  up1 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv4))\n",
        "  merge1 = concatenate([conv3,up1], axis = -1)\n",
        "  conv5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge1)\n",
        "                                                                                                  \n",
        "  up2 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv5))\n",
        "  merge2 = concatenate([conv2,up2], axis = -1)\n",
        "  conv6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge2)\n",
        "\n",
        "  up3 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "  merge3 = concatenate([conv1,up3], axis = -1)\n",
        "  conv7 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge3)\n",
        "\n",
        "  conv8 = Conv2D(1, 3,  padding = 'same')(conv7)\n",
        "\n",
        "  shallow_net = Model(inputs = inputs, outputs = conv8)\n",
        "\n",
        "  return shallow_net\n",
        "\n",
        "def get_denoise_model_add1(shape):\n",
        "  inputs = Input(shape)\n",
        "\n",
        "  conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "  conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "  pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "  conv3 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "  up1 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv3))\n",
        "  merge1 = concatenate([conv2,up1], axis = -1)\n",
        "  conv4 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge1)\n",
        "  \n",
        "  up2 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv4))\n",
        " \n",
        "  merge2 = concatenate([conv1,up2], axis = -1)\n",
        "  conv5 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge2)   \n",
        "\n",
        "  conv6 = Conv2D(1, 3,  padding = 'same')(conv5)\n",
        "  shallow_net = Model(inputs = inputs, outputs = conv6)\n",
        "\n",
        "  return shallow_net\n",
        "\n",
        "def get_denoise_model(shape):\n",
        "  inputs = Input(shape)\n",
        "  \n",
        "  ## Encoder starts\n",
        "  conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "  \n",
        "  ## Bottleneck\n",
        "  conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "\n",
        "  ## Now the decoder starts\n",
        "  up3 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv2))\n",
        "  #up3 = Conv2DTranspose(64, 2, strides=(2, 2), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "  merge3 = concatenate([conv1,up3], axis = -1)\n",
        "  conv3 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge3)\n",
        "  \n",
        "  conv4 = Conv2D(1, 3,  padding = 'same')(conv3)\n",
        "\n",
        "  shallow_net = Model(inputs = inputs, outputs = conv4)\n",
        "  \n",
        "  return shallow_net\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "\n",
        "def get_descriptor_model(shape):\n",
        "  \n",
        "  '''Architecture copies HardNet architecture'''\n",
        "  \n",
        "  init_weights = keras.initializers.he_normal()\n",
        "  \n",
        "  descriptor_model = Sequential()\n",
        "  descriptor_model.add(Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(32, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(64, 3, padding='same', strides=2, use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(64, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(128, 3, padding='same', strides=2,  use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(128, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "  descriptor_model.add(Dropout(0.3))\n",
        "\n",
        "  descriptor_model.add(Conv2D(128, 8, padding='valid', use_bias = True, kernel_initializer=init_weights))\n",
        "  \n",
        "  # Final descriptor reshape\n",
        "  descriptor_model.add(Reshape((128,)))\n",
        "  \n",
        "  return descriptor_model\n",
        "  \n",
        "  \n",
        "def triplet_loss(x):\n",
        "  \n",
        "  output_dim = 128\n",
        "  a, p, n = x\n",
        "  _alpha = 1.0\n",
        "  positive_distance = K.mean(K.square(a - p), axis=-1)\n",
        "  negative_distance = K.mean(K.square(a - n), axis=-1)\n",
        "  \n",
        "  return K.expand_dims(K.maximum(0.0, positive_distance - negative_distance + _alpha), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RlS5zcV7EJgp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Denoising Image Patches\n"
      ]
    },
    {
      "metadata": {
        "id": "wHxHwjUd3-pY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We use the *DenoiseHPatches* class implemented in the read_data.py file, which takes as input the list of sequences to load and the size of batches. \n",
        "\n",
        "*DenoiseHPatches* outputs batches where the input data is the noisy image and the label is the clean image, so we can use a mean absolute error (MAE) metric as loss function. You can try to use different metrics here to see if that improves results. \n",
        "\n",
        "Afterward, we take a subset of training and validation sequences by using *random.sample* (3 sequences for training and 1 for validation data). The purpose of doing so is just to speed-up training when trying different setups, but you should use the whole dataset when training your final model. Remove the random.sample function to give the generator all the training data.\n",
        "\n",
        "In addition, note that we are using the test set as validation. We will provide you with a new test set that will be used to evaluate your final model, and from which you will not have the clean images. \n",
        "\n",
        "**Updated**: Training should be quite faster now (1 epoch around 15 minutes)."
      ]
    },
    {
      "metadata": {
        "id": "m_VPSHmSK0dS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "random.seed(5)\n",
        "denoise_generator = DenoiseHPatches(random.sample(seqs_train, 3), batch_size=50)\n",
        "denoise_generator_val = DenoiseHPatches(random.sample(seqs_test, 1), batch_size=50)\n",
        "\n",
        "# Uncomment following lines for using all the data to train the denoising model\n",
        "# denoise_generator = DenoiseHPatches(seqs_train, batch_size=50)\n",
        "# denoise_generator_val = DenoiseHPatches(seqs_test, batch_size=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-eUSba93Dttj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shape = (32, 32, 1)\n",
        "denoise_model = get_denoise_model(shape)\n",
        "#denoise_model = get_denoise_model_add1(shape)\n",
        "#denoise_model = get_denoise_model_add2(shape)\n",
        "plot_model(denoise_model,\n",
        "    to_file='denoise_model.png',\n",
        "    show_shapes=True,\n",
        "    show_layer_names=False,\n",
        "    rankdir='TB')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H3wkjkpk4bRh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We set number of epochs to 1, tweak it, along with other hyperparameters, to improve the performance of the model."
      ]
    },
    {
      "metadata": {
        "id": "8q8H9oc2_m_W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#denoise_loss_sgd = np.empty([0,30])\n",
        "#denoise_val_loss_sgd = np.empty([0,30])\n",
        "#denoise_loss_RMSprop = np.empty([0,30])\n",
        "#denoise_val_loss_RMSprop = np.empty([0,30])\n",
        "denoise_loss_Adam = np.empty([0,30])\n",
        "denoise_val_loss_Adam = np.empty([0,30])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "edwbgE6yKqcD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Optimizers\n",
        "Adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "RMSprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "sgd = keras.optimizers.SGD(lr=0.00001, momentum=0.9, nesterov=True)\n",
        "\n",
        "denoise_model.compile(loss='mean_absolute_error', optimizer=Adam, metrics=['mae'])\n",
        "epochs = 1\n",
        "### Use a loop to save for each epoch the weights in an external website in\n",
        "### case colab stops. Every time you call fit/fit_generator the weigths are NOT\n",
        "### reset, so e.g. calling 5 times fit(epochs=1) behave as fit(epochs=5)\n",
        "for e in range(epochs):\n",
        "  denoise_history = denoise_model.fit_generator(generator=denoise_generator, \n",
        "                                                epochs=30, verbose=1, \n",
        "                                                validation_data=denoise_generator_val)\n",
        "  ### Saves optimizer and weights\n",
        "  denoise_model.save('denoise.h5') \n",
        "  ### Uploads files to external hosting\n",
        "  !curl -F \"file=@denoise.h5\" https://file.io\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sNCR4JIYnZpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#denoise_loss_sgd = np.append(denoise_loss_sgd, denoise_history.history['loss'])\n",
        "#denoise_val_loss_sgd = np.append(denoise_val_loss_sgd, denoise_history.history['val_loss'])\n",
        "\n",
        "#denoise_loss_RMSprop = np.append(denoise_loss_RMSprop, denoise_history.history['loss'])\n",
        "#denoise_val_loss_RMSprop = np.append(denoise_val_loss_RMSprop, denoise_history.history['val_loss'])\n",
        "\n",
        "denoise_loss_Adam = np.append(denoise_loss_Adam, denoise_history.history['loss'])\n",
        "denoise_val_loss_Adam = np.append(denoise_val_loss_Adam, denoise_history.history['val_loss'])\n",
        "\n",
        "plt.plot(denoise_loss_RMSprop,'b-', label=\"training loss with RMSprop\")\n",
        "plt.plot(denoise_val_loss_RMSprop,'b--', label=\"validation loss with RMSprop\")\n",
        "plt.plot(denoise_loss_Adam,'r-', label=\"training loss with Adam\")\n",
        "plt.plot(denoise_val_loss_Adam,'r--', label=\"validation loss with Adam\")\n",
        "plt.plot(denoise_loss_sgd,'g-', label=\"training loss with SGD\")\n",
        "plt.plot(denoise_val_loss_sgd,'g--', label=\"validation loss with SGD\")\n",
        "#plt.ylim(top=12)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2dKBM4qA8GTw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After every epoch, the code will generate an external link, this link saves your weights in case of colab disconnecting during training. Example of an epoch:\n",
        "\n",
        "**Epoch 1/1**\n",
        "1797/1797 [==============================] - 48s 27ms/step - loss: 11.4135 - \n",
        "mean_absolute_error: 11.4135 - val_loss: 7.6013 - val_mean_absolute_error: 7.6013 \n",
        "{\"success\":true,\"key\":\"fv9vjj\"\n",
        "\n",
        "\"link\":\"https://file.io/fv9vjj\",\"expiry\":\"14 days\"} **Epoch 1/1**"
      ]
    },
    {
      "metadata": {
        "id": "Ohb6Q94z4yya",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If colab did not disconnect, and you want to save the weights in your local disk, you also can use:\n"
      ]
    },
    {
      "metadata": {
        "id": "GjAQRnPV47BI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('denoise.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vCfE3xnF8Nfc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Moreover, if you have a model saved from a previous training session, you can upload it to colab and initialize the model's weights with it. \n",
        "\n",
        "You either can use `!wget download_link` or upload the weights from your local disk by using the left panel ('Files' section) in colab.\n",
        "\n",
        "Once the weights are uploaded, you can use\n",
        "\n",
        "> ``denoise_model = keras.models.load_model('./denoise.h5')\n",
        "``\n",
        "\n",
        "to load the weights."
      ]
    },
    {
      "metadata": {
        "id": "xMHY9CR16gjD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "denoise_model = keras.models.load_model('./denoise.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e9FzSZzMEcs4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualization of Denoising Results\n",
        "To visualize how the denoised patches look, you can run the following function. It returns the noisy patch, the denoised patch in the middle, and the clean patch in the right side. "
      ]
    },
    {
      "metadata": {
        "id": "XFA_8uN4Eb3B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_denoise(denoise_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Isx8_SpqFL6m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#delete generators used in the denoising model training\n",
        "del denoise_generator\n",
        "del denoise_generator_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SyABaCvkEPDR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training a Descriptor Network\n",
        "In the last section we trained a model that given a noisy patch, outputs a denoised version of it. We hoped that by doing so, we will improve the performance of the second part, which is training a network that outputs the descriptor. As we mentioned, a descriptor is a numerical vector that represents the small images we have. The dataset consists of a large number of small images, which are cropped patches from other larger images. Hence, they represent some local part of a scene. That is why there are no objects represented, only corners or textures. Each of these patches is related to a subset of other patches of the dataset by some kind of geometric transformation (e.g. rotation).  For a given patch, we want the network to output a vector that is close to the vectors of the patches that represent the same local part of a scene, while being far from patches do not represent that local part of a scene.\n",
        "\n",
        "To do so, we will build a convolutional neural network that takes the input of $32\\times32$ and outputs a descriptor of size $128$. For the loss, we use the triplet loss, which takes an anchor patch, a negative patch and a positive patch. The idea is to train the network so the descriptors from the anchor and positive patch have a low distance between them, and the negative and anchor patch have a large distance between them. \n",
        "\n",
        "In this cell we generate a triplet network, which is a network formed by three copies of the same network. That means that the descriptor model will compute the descriptor for the input `'a'` (anchor), the same descriptor model (with the same weights) will compute the descriptor for the input `'p'` (positive), and again the same model will compute the descriptor for the input `'n'` (negative). \n",
        "\n",
        "**Updated explanation**: Due to the way Keras handles the compile method, it needs a loss as an argument in that compile method. However, our loss is computed in the lambda layer, so we want to minimize the output of that layer. As we want to minimize the output of the Lambda function (in this case the triplet loss), we output as the label in the training_generator a vector of zeros and we compute the mean absolute error of the triplet loss and this vector of zeros. To give you an intuition, what we aim to minimize is\n",
        "$$  |\\text{triplet_loss} - 0| =  |\\text{triplet_loss}| = \\text{triplet_loss} $$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DX2HsY87WI0T",
        "colab_type": "code",
        "outputId": "57c42264-c2c1-477c-b6fb-e1c897bb63d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "cell_type": "code",
      "source": [
        "denoise_model = keras.models.load_model('./denoise.h5')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fz-ZkcoCSGnh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import layers, models\n",
        "img_height = 32\n",
        "img_width = 32\n",
        "img_channels = 1\n",
        "cardinality = 1\n",
        "\n",
        "\n",
        "def residual_network(x):\n",
        "    \"\"\"\n",
        "    ResNeXt by default. For ResNet set `cardinality` = 1 above.\n",
        "    \n",
        "    \"\"\"\n",
        "    def add_common_layers(y):\n",
        "        y = layers.BatchNormalization()(y)\n",
        "        y = layers.LeakyReLU()(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def grouped_convolution(y, nb_channels, _strides):\n",
        "        # when `cardinality` == 1 this is just a standard convolution\n",
        "        if cardinality == 1:\n",
        "            return layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
        "        \n",
        "        assert not nb_channels % cardinality\n",
        "        _d = nb_channels // cardinality\n",
        "\n",
        "        # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
        "        # and convolutions are separately performed within each group\n",
        "        groups = []\n",
        "        for j in range(cardinality):\n",
        "            group = layers.Lambda(lambda z: z[:, :, :, j * _d:j * _d + _d])(y)\n",
        "            groups.append(layers.Conv2D(_d, kernel_size=(3, 3), strides=_strides, padding='same')(group))\n",
        "            \n",
        "        # the grouped convolutional layer concatenates them as the outputs of the layer\n",
        "        y = layers.concatenate(groups)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def residual_block(y, nb_channels_in, nb_channels_out, _strides=(1, 1), _project_shortcut=False):\n",
        "        \"\"\"\n",
        "        Our network consists of a stack of residual blocks. These blocks have the same topology,\n",
        "        and are subject to two simple rules:\n",
        "\n",
        "        - If producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes).\n",
        "        - Each time the spatial map is down-sampled by a factor of 2, the width of the blocks is multiplied by a factor of 2.\n",
        "        \"\"\"\n",
        "        shortcut = y\n",
        "\n",
        "        # we modify the residual building block as a bottleneck design to make the network more economical\n",
        "        y = layers.Conv2D(nb_channels_in, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
        "        y = add_common_layers(y)\n",
        "\n",
        "        # ResNeXt (identical to ResNet when `cardinality` == 1)\n",
        "        y = grouped_convolution(y, nb_channels_in, _strides=_strides)\n",
        "        y = add_common_layers(y)\n",
        "\n",
        "        y = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
        "        # batch normalization is employed after aggregating the transformations and before adding to the shortcut\n",
        "        y = layers.BatchNormalization()(y)\n",
        "\n",
        "        # identity shortcuts used directly when the input and output are of the same dimensions\n",
        "        if _project_shortcut or _strides != (1, 1):\n",
        "            # when the dimensions increase projection shortcut is used to match dimensions (done by 1×1 convolutions)\n",
        "            # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
        "            shortcut = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
        "            shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "        y = layers.add([shortcut, y])\n",
        "\n",
        "        # relu is performed right after each batch normalization,\n",
        "        # expect for the output of the block where relu is performed after the adding to the shortcut\n",
        "        y = layers.LeakyReLU()(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "    # conv1\n",
        "    x = layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
        "    x = add_common_layers(x)\n",
        "\n",
        "    # conv2\n",
        "    x = layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
        "    for i in range(1):\n",
        "        project_shortcut = True if i == 0 else False\n",
        "        x = residual_block(x, 32, 64, _project_shortcut=project_shortcut)\n",
        "\n",
        "    # conv3\n",
        "    for i in range(1):\n",
        "        # down-sampling is performed by conv3_1, conv4_1, and conv5_1 with a stride of 2\n",
        "        strides = (2, 2) if i == 0 else (1, 1)\n",
        "        x = residual_block(x, 64, 128, _strides=strides)\n",
        "        \n",
        "    # conv4\n",
        "    for i in range(1):\n",
        "        # down-sampling is performed by conv3_1, conv4_1, and conv5_1 with a stride of 2\n",
        "        strides = (2, 2) if i == 0 else (1, 1)\n",
        "        x = residual_block(x, 128, 256, _strides=strides)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    #x = layers.Conv2D(128, kernel_size=(3, 3), strides=(8, 8), padding='same')(x)\n",
        "    \n",
        "    x = layers.Dense(128)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
        "network_output = residual_network(image_tensor)\n",
        "  \n",
        "model_res = models.Model(inputs=[image_tensor], outputs=[network_output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DVmDZIRTHPDa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Lambda\n",
        "shape = (32, 32, 1)\n",
        "xa = Input(shape=shape, name='a')\n",
        "xp = Input(shape=shape, name='p')\n",
        "xn = Input(shape=shape, name='n')\n",
        "descriptor_model = get_descriptor_model(shape)\n",
        "#descriptor_model = model_res\n",
        "ea = descriptor_model(xa)\n",
        "ep = descriptor_model(xp)\n",
        "en = descriptor_model(xn)\n",
        "\n",
        "loss = Lambda(triplet_loss)([ea, ep, en])\n",
        "\n",
        "descriptor_model_trip = Model(inputs=[xa, xp, xn], outputs=loss)\n",
        "#Optimizer\n",
        "sgd = keras.optimizers.SGD(lr=0.1)\n",
        "#Adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "#RMSprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "descriptor_model_trip.compile(loss='mean_absolute_error', optimizer=sgd)\n",
        "\n",
        "plot_model(descriptor_model,\n",
        "    to_file='descriptor_model.png',\n",
        "    show_shapes=True,\n",
        "    show_layer_names=False,\n",
        "    rankdir='TB')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BllXKocHCwZ7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we use the class HPatches, which loads the corresponding files by using the method `read_image_file`. It reads the patches. The output of read_image_file is a tuple of the form (images, labels), which is passed to the class `DataGeneratorDesc`. This class is a generator that creates batches of triplets, and each epoch is defined by the number of triplets in the argument `num_triplets`.\n",
        "\n",
        "**Updated**: In the previous version of the baseline code, we were training the descriptor model with the noisy patches, not with the denoised ones. By adding the argument `denoise_model=denoise_model` to the class HPatches we can use the denoised images instead to train this descriptor model (if `denoise_model=None`, the noisy patches will be used). However, as it has to compute the denoised patch first, the loading of the data will be slower (6/7 extra min).  If you want to train the model with the clean patches instead, you can set the argument `use_clean=True`. In this last case, even if a denoise model is given, it will not be used. When running this piece of code **the type of patches (denoised, noisy or clean) used is printed**."
      ]
    },
    {
      "metadata": {
        "id": "YIR1cH4fDwKj",
        "colab_type": "code",
        "outputId": "1b3ac79c-54f1-4a44-d42d-25871473ddec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "cell_type": "code",
      "source": [
        "### Descriptor loading and training\n",
        "# Loading images\n",
        "hPatches = HPatches(train_fnames=train_fnames, test_fnames=test_fnames,\n",
        "                    denoise_model=denoise_model, use_clean=False)\n",
        "\n",
        "# Creating training generator\n",
        "#training_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=1), num_triplets=20000)\n",
        "# Creating validation generator\n",
        "#val_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=0), num_triplets=2000)\n",
        "\n",
        "training_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=1), num_triplets=100000)\n",
        "val_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=0), num_triplets=10000)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using denoised patches\n",
            "100%|██████████| 116/116 [00:34<00:00,  3.56it/s]\n",
            "Denoising patches...\n",
            "100%|██████████| 15589/15589 [11:40<00:00, 22.22it/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:01<00:00, 54802.63it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using denoised patches\n",
            "100%|██████████| 116/116 [00:20<00:00,  5.70it/s]\n",
            "Denoising patches...\n",
            "100%|██████████| 9525/9525 [07:06<00:00, 22.31it/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 61143.78it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "GoQYyuD7_4PS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We plot a random triplet in the form of anchor, positive and negative sample. The positive and anchor patches are similar between them (the difference is a geometric transformation, for example rotation), whereas the negative sample should be quite dissimilar to any of the other two."
      ]
    },
    {
      "metadata": {
        "id": "3RQmOMU92csu",
        "colab_type": "code",
        "outputId": "8c4b5799-c5c3-4104-b9a3-9ae842762ef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "cell_type": "code",
      "source": [
        "plot_triplet(training_generator)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcwAAACmCAYAAABXw78OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "UaE2_6HUCAOw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now train the descriptor model and save the weights afterward."
      ]
    },
    {
      "metadata": {
        "id": "HvZ0MtPvTex8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#descriptor_loss_sgd = np.empty([0,1])\n",
        "#descriptor_val_loss_sgd = np.empty([0,1])\n",
        "#descriptor_loss_RMSprop = np.empty([0,1])\n",
        "#descriptor_val_loss_RMSprop = np.empty([0,1])\n",
        "#descriptor_loss_Adam = np.empty([0,1])\n",
        "#descriptor_val_loss_Adam = np.empty([0,1])\n",
        "\n",
        "#descriptor_loss_res = np.empty([0,1])\n",
        "#descriptor_val_loss_res = np.empty([0,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QPyc8as42WTQ",
        "colab_type": "code",
        "outputId": "7ec91aa4-da35-4727-82ee-40d9e498826a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "### As with the denoising model, we use a loop to save for each epoch \n",
        "## #the weights in an external website in case colab stops. \n",
        "### reset, so e.g. calling 5 times fit(epochs=1) behave as fit(epochs=5)\n",
        "\n",
        "### If you have a model saved from a previous training session\n",
        "### Load it in the next line\n",
        "# descriptor_model_trip.set_weights(keras.models.load_model('./descriptor.h5').get_weights())\n",
        "# descriptor_model_trip.optimizer = keras.models.load_model('./descriptor.h5').optimizer\n",
        "\n",
        "for e in range(epochs):\n",
        "  \n",
        "  descriptor_history = descriptor_model_trip.fit_generator(generator=training_generator, epochs=1, verbose=1, validation_data=val_generator)\n",
        "  \n",
        "  #descriptor_val_loss_sgd = np.append(descriptor_val_loss_sgd, descriptor_history.history['val_loss'])\n",
        "  #descriptor_loss_sgd = np.append(descriptor_loss_sgd, descriptor_history.history['loss'])\n",
        "  #descriptor_val_loss_RMSprop = np.append(descriptor_val_loss_RMSprop, descriptor_history.history['val_loss'])\n",
        "  #descriptor_loss_RMSprop = np.append(descriptor_loss_RMSprop, descriptor_history.history['loss'])\n",
        "  #descriptor_val_loss_Adam = np.append(descriptor_val_loss_Adam, descriptor_history.history['val_loss'])\n",
        "  #descriptor_loss_Adam = np.append(descriptor_loss_Adam, descriptor_history.history['loss'])\n",
        "  \n",
        "  #descriptor_val_loss_res = np.append(descriptor_val_loss_res, descriptor_history.history['val_loss'])\n",
        "  #descriptor_loss_res = np.append(descriptor_loss_res, descriptor_history.history['loss'])\n",
        "  \n",
        "  ### Saves optimizer and weights\n",
        "  descriptor_model_trip.save('descriptor.h5') \n",
        "  ### Uploads files to external hosting\n",
        "  !curl -F \"file=@descriptor.h5\" https://file.io\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.2176"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:03<00:00, 29030.78it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 166s 83ms/step - loss: 0.2176 - val_loss: 0.2685\n",
            "{\"success\":true,\"key\":\"e9TYGT\",\"link\":\"https://file.io/e9TYGT\",\"expiry\":\"14 days\"}Epoch 1/1\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.1586"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 96559/100000 [00:03<00:00, 31628.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2000/2000 [==============================] - 162s 81ms/step - loss: 0.1586 - val_loss: 0.2389\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:03<00:00, 27950.86it/s]\n",
            "100%|██████████| 10000/10000 [00:00<00:00, 59991.22it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"success\":true,\"key\":\"BIer4J\",\"link\":\"https://file.io/BIer4J\",\"expiry\":\"14 days\"}Epoch 1/1\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.1444"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:04<00:00, 23572.68it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2000/2000 [==============================] - 162s 81ms/step - loss: 0.1444 - val_loss: 0.1655\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 60182.31it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"success\":true,\"key\":\"fScw3A\",\"link\":\"https://file.io/fScw3A\",\"expiry\":\"14 days\"}Epoch 1/1\n",
            "1177/2000 [================>.............] - ETA: 1:03 - loss: 0.1330"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gz12Z6-bs7h5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(descriptor_loss_RMSprop,'b-', label=\"training loss with RMSprop\")\n",
        "plt.plot(descriptor_val_loss_RMSprop,'b--', label=\"validation loss with RMSprop\")\n",
        "plt.plot(descriptor_loss_Adam,'r-', label=\"training loss with Adam\")\n",
        "plt.plot(descriptor_val_loss_Adam,'r--', label=\"validation loss with Adam\")\n",
        "plt.plot(descriptor_loss_sgd,'g-', label=\"training loss with SGD\")\n",
        "plt.plot(descriptor_val_loss_sgd,'g--', label=\"validation loss with SGD\")\n",
        "\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NJ-r9D4hDxij",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generating descriptors files for test data \n",
        "\n",
        "To evaluate the performance of out model we will use an existing evaluation code, which is called HPatches benchmark. HPatches benchmark takes as input the descriptors for the test data in a CSV form. So the whole pipeline is represented in the following image.\n",
        "\n",
        "![](https://i.ibb.co/WcDDf3q/Screenshot-from-2019-02-15-11-17-24.png)\n",
        "\n",
        "This function generates those files by passing it a descriptor model and a denoising model. It performs a first step of denoising the patches, and a second one of computing the descriptor of the denoised patch. If no denoising model is given (variable set to `None`), the descriptor is computed directly in the noisy patch.\n",
        "\n",
        "Similarly to the loading data part, you have the denoise_model variable and `use_clean` variable. If `use_clean` is set to True, the CSV generated will be those of the clean patches, even if a denoising model is given. If set to False, then depends on the variable `denoise_model`. If there is no denoise model (`denoise_model=None`), then it will use the noisy patches. If you give a denoising model, then it will compute the CSV for the denoised patches. This can be useful to explore different scenarios (for example, the Upper Bound can be training the descriptor network with clean patches, and testing with clean patches), however you should always report the score when using noisy patches (depending on the approach you develop, you may want to denoise them or not). The official baseline uses the denoised patches. "
      ]
    },
    {
      "metadata": {
        "id": "kiJb2XDG9bsJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate_desc_csv(descriptor_model, seqs_test, denoise_model=denoise_model, use_clean=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s0jFr05rE1oI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluating descriptors in HPatches Benchmark\n",
        "We use HPatches benchmark code to compute the results for our model. \n",
        "\n",
        "**Updated**: The necessary code is included in the repository we cloned at the beginning of the code, so we do not need to download any extra data. Also, we simplified the results, so now they only return one value for each of the three tasks."
      ]
    },
    {
      "metadata": {
        "id": "YvOGRh3sc9Wo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we will perform the evaluation of three different tasks (Verification, Matching and Evaluation) using the CSV files we generated as input and the `hpatches_eval.py` script. We also print the results using the `hpatches_results.py` script. The scripts will return a score for each of the tasks. The metric used is called mean Average Precision, which it uses the Precision of the model. The Precision is defined, for a given number of retrieved elements, as the ratio of correct retrieved elements / number of retrieved elements. [Link to Wikipedia with Precision explanation](https://en.wikipedia.org/wiki/Precision_and_recall). The definition of the three different tasks is taken from the [HPatches paper](https://arxiv.org/pdf/1704.05939.pdf).\n",
        "\n",
        "In all of the tasks if you use the optional argument `--more_info` in `hpatches_results.py` you can see extra mAP information. However, the important score is the mAP score reported without this flag.\n",
        "\n",
        "### Verification\n",
        "\n",
        "Patch verification measures the ability of a descriptor to classify whether two patches are extracted from the same measurement. Now we compute the score of our architecture in this task.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Awnyv4xTYSFH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/keras_triplet_descriptor/out/ --task=verification --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=verification\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5290Bw-udJdr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Matching\n",
        "Image matching, tests to what extent a descriptor can correctly identify correspondences in two images."
      ]
    },
    {
      "metadata": {
        "id": "EUqpwi87ckJv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/keras_triplet_descriptor/out/ --task=matching --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=matching\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RXXgbN7DdMnx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Retrieval\n",
        "Retrieval tests how well a descriptor can match a query patch to a pool of patches extracted from many images."
      ]
    },
    {
      "metadata": {
        "id": "ZNmKIat1cn_M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/keras_triplet_descriptor/out/ --task=retrieval --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=retrieval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8_2fBzUB5RF2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Compressing and saving the CSV files \n",
        "\n",
        "This is not necessary for the analysis of the baseline code included in the report. However, we will be hosting a competition in an external website to see who can achieve the highest score. In that case, you will need to submit the CSV files, as the scoring script will be performed in an external server. With that aim, we include here a way to save the files either in your local disc or in your google drive account.\n",
        "\n",
        "We first compress the directory with all the CSV by using the following command. Remove the `q` option if you want it to output the progress."
      ]
    },
    {
      "metadata": {
        "id": "Lh_svT3p5Ww-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!zip -rq descriptors.zip ./out/custom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "svoL779J8AJK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The generated .zip is quite large, the method we used for the weights does not work. We have two other methods. First, in the file explorer in the left column we can right-click in the file and then click download. Then, we will see a circle next to the file showing the download progress.\n",
        "\n",
        "The second way does not require for you to download the files, it save the zip file in your Google Drive account, and you can download it later to your machine if you want. To do so, follow this method (found [here](https://stackoverflow.com/questions/49428332/how-to-download-large-files-like-weights-of-a-model-from-colaboratory)). First run the next cell, and the output will be a link for authentication purposes, and just follow the instructions"
      ]
    },
    {
      "metadata": {
        "id": "RjOmPv5z7Opx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "def save_file_to_drive(name, path):\n",
        "  file_metadata = {\n",
        "    'name': name,\n",
        "    'mimeType': 'application/octet-stream'\n",
        "  }\n",
        "\n",
        "  media = MediaFileUpload(path, \n",
        "                          mimetype='application/octet-stream',\n",
        "                          resumable=True)\n",
        "\n",
        "  created = drive_service.files().create(body=file_metadata,\n",
        "                                  media_body=media,\n",
        "                                  fields='id').execute()\n",
        "\n",
        "  print('File ID: {}'.format(created.get('id')))\n",
        "\n",
        "  return created\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YfzjfMc59NKm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can use the following function to save the file to your drive account. The second argument is the name of the file we want to save, and the first argument the name that will have in your Drive."
      ]
    },
    {
      "metadata": {
        "id": "UwrqWr_c7pAi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "save_file_to_drive('descriptors_save.zip', 'descriptors.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}